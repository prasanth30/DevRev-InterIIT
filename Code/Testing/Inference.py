# -*- coding: utf-8 -*-
"""Inter IIT NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hHevGoN2ayvB8mCjhJ79mHSieULXX5uZ
"""

!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TextStreamer
import torch
import os
import pandas as pd

df = pd.read_csv('/content/data.csv')

df

class LlamaChatBot:
    def __init__(self, model_name ='dhruvabansal/llama-2-13b'):
        torch.cuda.empty_cache()
        self.isGPU = torch.cuda.is_available()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.isGPU:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
            )
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)

    def generate_response(self, prompt):
        if self.isGPU:
            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)
        else: input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
        streamer = TextStreamer(self.tokenizer) #text streamer initialized using tokenizer
        generated_ids = self.model.generate(input_ids, streamer=streamer, max_length=500) #streams output on "generate" function call
        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True) #storing the output regardless
        return generated_text

predictions_list = []

# Commented out IPython magic to ensure Python compatibility.
# %%time
# if __name__ == "__main__":
#     chatbot = LlamaChatBot()
#     for i in range(0,92):
#       prompt = df['query'][i]
#       response = chatbot.generate_response(prompt)
#       predictions_list.append(response)

predictions_list

df['LLama_output'] = predictions_list

predictions_list[0]

df.to_csv('temp.csv')

df

